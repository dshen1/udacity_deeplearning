{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Wiki Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download data from https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mini-demo\n",
    "from urllib.request import urlretrieve\n",
    "import os \n",
    "from os.path import isfile, isdir\n",
    "import zipfile \n",
    "from tqdm import tqdm\n",
    "import numpy as np #vectorization\n",
    "import random #generate probability distribution \n",
    "import tensorflow as tf #ml\n",
    "import datetime #clock training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First download data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exist\n"
     ]
    }
   ],
   "source": [
    "#### process bar\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "## download file \n",
    "data_path = './wikitext'\n",
    "if isdir(data_path):\n",
    "    print('Data already exist')\n",
    "else:\n",
    "    if not isdir(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    zip_file = os.path.join(data_path,'wikitext-103-v1.zip')\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='wikidata') as pbar:\n",
    "        #urlretrieve('https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip',\n",
    "        #            zip_file,\n",
    "        #            pbar.hook)\n",
    "        urlretrieve('https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip',\n",
    "                    zip_file,\n",
    "                    pbar.hook)\n",
    "    with zipfile.ZipFile(os.path.join(data_path,'wikitext-103-v1.zip')) as myzip:\n",
    "        myzip.extractall(data_path)\n",
    "    ## remove zip file \n",
    "    os.remove(data_path+'/wikitext-103-v1.zip')\n",
    "\n",
    "data_file_path = \"./wikitext/wikitext-2\"\n",
    "train_file = os.path.join(data_file_path,'wiki.train.tokens')\n",
    "validate_file = os.path.join(data_file_path,'wiki.valid.tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length in number of characters: 500000\n",
      "head of text:\n",
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Char\n"
     ]
    }
   ],
   "source": [
    "#lets open the text\n",
    "#native python file read function\n",
    "text = open(train_file,encoding='utf8').read()\n",
    "text = text[:500000]\n",
    "print('text length in number of characters:', len(text))\n",
    "print('head of text:')\n",
    "print(text[:1000]) #all tokenized words, stored in a list called text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a id to character and character to id map dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters: 127\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '+', ',', '-', '.', '/', '0', '1', '2', '3']\n"
     ]
    }
   ],
   "source": [
    "## get the set of characters and sort them \n",
    "chars = sorted(list(set(text)))               ## all unique characters\n",
    "char_size = len(chars)\n",
    "print('number of characters:', char_size)\n",
    "print(chars[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## chrate char to id and id to char map \n",
    "char2id = {c:i for i,c in enumerate(chars)}\n",
    "id2char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_num = [char2id[s] for s in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a probability of each character, return a likely character, one-hot encoded\n",
    "#our prediction will give us an array of probabilities of each character\n",
    "#we'll pick the most likely and one-hot encode it\n",
    "def sample(prediction):\n",
    "    '''\n",
    "    prediction: is a list of characters probilities\n",
    "    '''\n",
    "    r = random.uniform(0,1)  ## it is just a random number from 0-1\n",
    "    s = 0 \n",
    "    char_id = len(prediction)-1  ## this is because it starts with 0\n",
    "    #for each char prediction probability \n",
    "    for i in range(len(prediction)):\n",
    "        s+= prediction[i]\n",
    "        if s >= r:\n",
    "            char_id = i \n",
    "            break \n",
    "    \n",
    "    char_one_hot = np.zeros(shape[char_size])  ## one hot encode characters \n",
    "    char_one_hot[char_id] = 1.0\n",
    "    return char_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X and y sets and one hot encode them  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def one_hot_encode(x,label_binarizer):\n",
    "    return label_binarizer.transform(x).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## \n",
    "#vectorize our data to feed it into model\n",
    "len_per_section = 50\n",
    "skip = 2\n",
    "sections = []\n",
    "next_chars = []\n",
    "#fill sections list with chunks of text, every 2 characters create a new 50 \n",
    "#character long section\n",
    "#because we are generating it at a character level\n",
    "for i in range(0, len(text_num) - len_per_section, skip):\n",
    "    sections.append(text_num[i: i + len_per_section])\n",
    "    next_chars.append(text_num[i + len_per_section])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(249975, 50, 127) (249975, 127)\n"
     ]
    }
   ],
   "source": [
    "label_binarizer = LabelBinarizer()\n",
    "label_binarizer.fit(range(char_size))\n",
    "\n",
    "X = np.array([one_hot_encode(section,label_binarizer) for section in sections])\n",
    "y = np.array(one_hot_encode(next_chars,label_binarizer))\n",
    "print(X.shape,y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is not very efficient, need to figure out soemthing else "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data size: 249975\n",
      "approximate steps per epoch: 244\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024\n",
    "max_steps = 70000\n",
    "log_every = 1000\n",
    "save_every = 10000      \n",
    "hidden_nodes = 1024    ## number of hidden nodes \n",
    "test_start = 'i am thinkg that'\n",
    "checkpoint_directory = 'ckpt'\n",
    "\n",
    "## create a checkpoint directory \n",
    "if tf.gfile.Exists(checkpoint_directory):\n",
    "    tf.gfile.DeleteRecursively(checkpoint_directory)\n",
    "tf.gfile.MakeDirs(checkpoint_directory)\n",
    "\n",
    "print('training data size:', len(X))\n",
    "print('approximate steps per epoch:', int(len(X)/batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    #global_step just keeps track \n",
    "    #of the number of batches seen so far starts off as 0\n",
    "    global_step = tf.Variable(0)\n",
    "    #input data \n",
    "    data = tf.placeholder(tf.float32, [batch_size, len_per_section, char_size])\n",
    "    #labels\n",
    "    labels = tf.placeholder(tf.float32, [batch_size, char_size])\n",
    "    \n",
    "    ############################\n",
    "    #Now let's build LSTM Cell##\n",
    "    ############################\n",
    "    \n",
    "    #input gate \n",
    "    w_ii = tf.Variable(tf.truncated_normal([char_size,hidden_nodes],-0.1,0.1))\n",
    "    w_io = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_i = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Forget gate: weights for input, weights for previous output, and bias\n",
    "    w_fi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_fo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_f = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Output gate: weights for input, weights for previous output, and bias\n",
    "    w_oi = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_oo = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_o = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    #Memory cell: weights for input, weights for previous output, and bias\n",
    "    w_ci = tf.Variable(tf.truncated_normal([char_size, hidden_nodes], -0.1, 0.1))\n",
    "    w_co = tf.Variable(tf.truncated_normal([hidden_nodes, hidden_nodes], -0.1, 0.1))\n",
    "    b_c = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    #LSTM Cell\n",
    "    # given input, output, external state, it will return output and state\n",
    "    #output starts off empty, LSTM cell calculates it\n",
    "    \n",
    "    #Since, we have two kinds of states - the internal state ct \n",
    "    #and the (exposed) external state st, and since we need both of \n",
    "    #them for the subsequent sequential operations, we combine them \n",
    "    #into a tensor at each step, and pass them as input to the next \n",
    "    #step. This tensor is unpacked into st_1 and ct_1 at the beginning of each step.\n",
    "    \n",
    "    \n",
    "    def lstm(i, o, state):\n",
    "        \n",
    "        #these are all calculated seperately, no overlap until....\n",
    "        #(input * input weights) + (output * weights for previous output) + bias\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, w_ii) + tf.matmul(o, w_io) + b_i)\n",
    "        #(input * forget weights) + (output * weights for previous output) + bias\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, w_fi) + tf.matmul(o, w_fo) + b_f)\n",
    "        #(input * output weights) + (output * weights for previous output) + bias\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, w_oi) + tf.matmul(o, w_oo) + b_o)\n",
    "        #(input * internal state weights) + (output * weights for previous output) + bias\n",
    "        memory_cell = tf.sigmoid(tf.matmul(i, w_ci) + tf.matmul(o, w_co) + b_c)\n",
    "        \n",
    "        #...now! multiply forget gate * given state    +  input gate * hidden state\n",
    "        state = forget_gate * state + input_gate * memory_cell\n",
    "        #squash that state with tanh nonlin (Computes hyperbolic tangent of x element-wise)\n",
    "        #multiply by output\n",
    "        output = output_gate * tf.tanh(state)\n",
    "        #return \n",
    "        return output, state\n",
    "    \n",
    "    \n",
    "    ###########\n",
    "    #Operation\n",
    "    ###########\n",
    "    #LSTM\n",
    "    #both start off as empty, LSTM will calculate this\n",
    "    output = tf.zeros([batch_size, hidden_nodes])\n",
    "    state = tf.zeros([batch_size, hidden_nodes])\n",
    "\n",
    "    ####################################\n",
    "    ## Ok, this is the important part, LSTM is actually running a loop \n",
    "    ## it will slice each plane of the 3d matrix of input then do the \n",
    "    ## matrix multiplication \n",
    "    ####################################\n",
    "    for i in range(len_per_section):\n",
    "        #calculate state and output from LSTM\n",
    "        output, state = lstm(data[:, i, :], output, state)\n",
    "        #to start, \n",
    "        if i == 0:\n",
    "            #store initial output and labels\n",
    "            outputs_all_i = output\n",
    "            labels_all_i = data[:, i+1, :]\n",
    "        #for each new set, concat outputs and labels\n",
    "        elif i != len_per_section - 1:\n",
    "            #concatenates (combines) vectors along a dimension axis, not multiply\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output],0)        ## tf 1.0 changes\n",
    "            labels_all_i = tf.concat([labels_all_i, data[:, i+1, :]],0) ## tf 1.0 changes\n",
    "        else:\n",
    "            #final store\n",
    "            outputs_all_i = tf.concat([outputs_all_i, output],0)        ## tf 1.0 changes\n",
    "            labels_all_i = tf.concat([labels_all_i, labels],0)          ## tf 1.0 changes\n",
    "\n",
    "    #Classifier\n",
    "    #The Classifier will only run after saved_output and saved_state were assigned.\n",
    "    \n",
    "    #calculate weight and bias values for the network\n",
    "    #generated randomly given a size and distribution\n",
    "    w = tf.Variable(tf.truncated_normal([hidden_nodes, char_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([char_size]))\n",
    "    #Logits simply means that the function operates on the unscaled output \n",
    "    #of earlier layers and that the relative scale to understand the units \n",
    "    #is linear. It means, in particular, the sum of the inputs may not equal 1, \n",
    "    #that the values are not probabilities (you might have an input of 5).\n",
    "    logits = tf.matmul(outputs_all_i, w) + b\n",
    "    \n",
    "    #logits is our prediction outputs, lets compare it with our labels\n",
    "    #cross entropy since multiclass classification\n",
    "    #computes the cost for a softmax layer\n",
    "    #then Computes the mean of elements across dimensions of a tensor.\n",
    "    #average loss across all values\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels_all_i))\n",
    "\n",
    "    #Optimizer\n",
    "    #minimize loss with graident descent, learning rate 10,  keep track of batches\n",
    "    optimizer = tf.train.GradientDescentOptimizer(10.).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    ##########################################################\n",
    "    #Test ####################################################\n",
    "    ##########################################################\n",
    "    \n",
    "    ## it seems that for testing, we don't limit ourself to sequence length \n",
    "    ## it seems that outcome and states just keeps accumulateing \n",
    "    \n",
    "    test_data = tf.placeholder(tf.float32, shape=[1, char_size])\n",
    "    test_output = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    test_state = tf.Variable(tf.zeros([1, hidden_nodes]))\n",
    "    \n",
    "    Reset at the beginning of each test\n",
    "    reset_test_state = tf.group(test_output.assign(tf.zeros([1, hidden_nodes])), \n",
    "                                test_state.assign(tf.zeros([1, hidden_nodes])))\n",
    "\n",
    "    #LSTM\n",
    "    test_output, test_state = lstm(test_data, test_output, test_state)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(test_output, w) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we are going to train the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss at step 0: 4.89 (2017-05-31 21:36:36.931236)\n",
      "training loss at step 1000: 2.88 (2017-05-31 21:42:13.964942)\n",
      "training loss at step 2000: 2.56 (2017-05-31 21:47:51.331521)\n",
      "training loss at step 3000: 2.37 (2017-05-31 21:53:31.012398)\n",
      "training loss at step 4000: 1.95 (2017-05-31 21:59:10.962961)\n",
      "training loss at step 5000: 1.95 (2017-05-31 22:04:50.373679)\n",
      "training loss at step 6000: 1.83 (2017-05-31 22:10:30.394437)\n",
      "training loss at step 7000: 1.77 (2017-05-31 22:16:10.889125)\n",
      "training loss at step 8000: 1.82 (2017-05-31 22:21:51.179532)\n",
      "training loss at step 9000: 1.67 (2017-05-31 22:27:31.031314)\n",
      "training loss at step 10000: 1.62 (2017-05-31 22:32:50.144931)\n",
      "training loss at step 11000: 1.62 (2017-05-31 22:38:01.082995)\n",
      "training loss at step 12000: 1.60 (2017-05-31 22:43:10.854862)\n",
      "training loss at step 13000: 1.62 (2017-05-31 22:48:20.472440)\n",
      "training loss at step 14000: 1.43 (2017-05-31 22:53:30.762236)\n",
      "training loss at step 15000: 1.49 (2017-05-31 22:58:40.952771)\n",
      "training loss at step 16000: 1.70 (2017-05-31 23:03:50.541577)\n",
      "training loss at step 17000: 1.52 (2017-05-31 23:09:00.073728)\n",
      "training loss at step 18000: 1.56 (2017-05-31 23:14:09.753617)\n",
      "training loss at step 19000: 1.53 (2017-05-31 23:19:19.542368)\n",
      "training loss at step 20000: 1.45 (2017-05-31 23:24:29.131987)\n",
      "training loss at step 21000: 1.48 (2017-05-31 23:29:39.822157)\n",
      "training loss at step 22000: 1.51 (2017-05-31 23:34:49.318285)\n",
      "training loss at step 23000: 1.48 (2017-05-31 23:39:58.955888)\n",
      "training loss at step 24000: 1.38 (2017-05-31 23:45:08.566987)\n",
      "training loss at step 25000: 1.40 (2017-05-31 23:50:17.967602)\n",
      "training loss at step 26000: 1.25 (2017-05-31 23:55:27.559486)\n",
      "training loss at step 27000: 1.31 (2017-06-01 00:00:37.057081)\n",
      "training loss at step 28000: 1.25 (2017-06-01 00:05:46.596780)\n",
      "training loss at step 29000: 1.53 (2017-06-01 00:10:56.246197)\n",
      "training loss at step 30000: 1.36 (2017-06-01 00:16:05.903516)\n",
      "training loss at step 31000: 1.36 (2017-06-01 00:21:16.081407)\n",
      "training loss at step 32000: 1.43 (2017-06-01 00:26:25.854681)\n",
      "training loss at step 33000: 1.43 (2017-06-01 00:31:35.520438)\n",
      "training loss at step 34000: 1.33 (2017-06-01 00:36:45.037345)\n",
      "training loss at step 35000: 1.28 (2017-06-01 00:41:54.805080)\n",
      "training loss at step 36000: 1.29 (2017-06-01 00:47:04.551363)\n",
      "training loss at step 37000: 1.38 (2017-06-01 00:52:13.902618)\n",
      "training loss at step 38000: 1.27 (2017-06-01 00:57:23.557366)\n",
      "training loss at step 39000: 1.45 (2017-06-01 01:02:33.247684)\n",
      "training loss at step 40000: 1.34 (2017-06-01 01:07:42.934716)\n",
      "training loss at step 41000: 1.31 (2017-06-01 01:12:53.147127)\n",
      "training loss at step 42000: 1.23 (2017-06-01 01:18:02.765796)\n",
      "training loss at step 43000: 1.33 (2017-06-01 01:23:12.364233)\n",
      "training loss at step 44000: 1.29 (2017-06-01 01:28:22.015683)\n",
      "training loss at step 45000: 1.20 (2017-06-01 01:33:31.630101)\n",
      "training loss at step 46000: 1.43 (2017-06-01 01:38:41.004003)\n",
      "training loss at step 47000: 1.25 (2017-06-01 01:43:50.483848)\n",
      "training loss at step 48000: 1.33 (2017-06-01 01:49:00.131656)\n",
      "training loss at step 49000: 1.33 (2017-06-01 01:54:09.749743)\n",
      "training loss at step 50000: 1.35 (2017-06-01 01:59:19.400318)\n",
      "training loss at step 51000: 1.28 (2017-06-01 02:04:29.528149)\n",
      "training loss at step 52000: 1.30 (2017-06-01 02:09:39.071331)\n",
      "training loss at step 53000: 1.10 (2017-06-01 02:14:48.627669)\n",
      "training loss at step 54000: 1.28 (2017-06-01 02:19:58.050767)\n",
      "training loss at step 55000: 1.17 (2017-06-01 02:25:07.431137)\n",
      "training loss at step 56000: 1.18 (2017-06-01 02:30:16.922660)\n",
      "training loss at step 57000: 1.11 (2017-06-01 02:35:26.471250)\n",
      "training loss at step 58000: 1.17 (2017-06-01 02:40:35.858616)\n",
      "training loss at step 59000: 1.25 (2017-06-01 02:45:45.331545)\n",
      "training loss at step 60000: 1.20 (2017-06-01 02:50:54.882708)\n",
      "training loss at step 61000: 1.28 (2017-06-01 02:56:04.977104)\n",
      "training loss at step 62000: 1.32 (2017-06-01 03:01:14.369598)\n",
      "training loss at step 63000: 1.17 (2017-06-01 03:06:23.878808)\n",
      "training loss at step 64000: 1.29 (2017-06-01 03:11:33.424330)\n",
      "training loss at step 65000: 1.24 (2017-06-01 03:16:42.860147)\n",
      "training loss at step 66000: 1.19 (2017-06-01 03:21:52.171252)\n",
      "training loss at step 67000: 1.14 (2017-06-01 03:27:01.674078)\n",
      "training loss at step 68000: 1.13 (2017-06-01 03:32:11.192564)\n",
      "training loss at step 69000: 1.25 (2017-06-01 03:37:20.626727)\n"
     ]
    }
   ],
   "source": [
    "## initialize tf session \n",
    "with tf.Session(graph = graph) as sess:\n",
    "    ## intialize all variables \n",
    "    tf.global_variables_initializer().run()\n",
    "    offset = 0 \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    ##for each training step\n",
    "    for step in range(max_steps):\n",
    "        ## starts off as 0 \n",
    "        offset = offset % len(X)  ## the reminder \n",
    "        ## calculate batch data and lables to feed model iteratively \n",
    "        if offset <= (len(X)-batch_size):\n",
    "            # first part \n",
    "            batch_data = X[offset:offset+batch_size]\n",
    "            batch_labels = y[offset:offset+batch_size]\n",
    "            offset += batch_size\n",
    "        # until when offset = batch size, then we \n",
    "        else:\n",
    "            #last part \n",
    "            to_add = batch_size - (len(X) - offset)\n",
    "            batch_data = np.concatenate((X[offset:len(X)],X[0:to_add]))\n",
    "            batch_labels = np.concatenate((y[offset:len(X)],y[0:to_add]))\n",
    "            offset = to_add\n",
    "        \n",
    "        ## optimize!\n",
    "        _,training_loss = sess.run([optimizer,loss],feed_dict={data:batch_data,labels:batch_labels})\n",
    "        if step % log_every == 0:\n",
    "            print('training loss at step %d: %.2f (%s)' % (step, training_loss, datetime.datetime.now()))\n",
    "            if step % save_every == 0:\n",
    "                saver.save(sess, checkpoint_directory + '/model', global_step=step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we can use the model to predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start = 'I plan to make the world a better place '\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    #init graph, load model\n",
    "    tf.global_variables_initializer().run()\n",
    "    model = tf.train.latest_checkpoint(checkpoint_directory)\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, model)\n",
    "\n",
    "    #set input variable to generate chars from\n",
    "    reset_test_state.run() \n",
    "    test_generated = test_start\n",
    "\n",
    "    #for every char in the input sentennce\n",
    "    for i in range(len(test_start) - 1):\n",
    "        #initialize an empty char store\n",
    "        test_X = np.zeros((1, char_size))\n",
    "        #store it in id from, onehot encode it \n",
    "        test_X[0, char2id[test_start[i]]] = 1.\n",
    "        #feed it to model, test_prediction is the output value\n",
    "        _ = sess.run(test_prediction, feed_dict={test_data: test_X})\n",
    "\n",
    "    \n",
    "    #where we store encoded char predictions\n",
    "    test_X = np.zeros((1, char_size))\n",
    "    test_X[0, char2id[test_start[-1]]] = 1.\n",
    "\n",
    "    #lets generate 500 characters\n",
    "    for i in range(500):\n",
    "        #get each prediction probability\n",
    "        prediction = test_prediction.eval({test_data: test_X})[0]\n",
    "        #one hot encode it\n",
    "        next_char_one_hot = sample(prediction)\n",
    "        #get the indices of the max values (highest probability)  and convert to char\n",
    "        next_char = id2char[np.argmax(next_char_one_hot)]\n",
    "        #add each char to the output text iteratively\n",
    "        test_generated += next_char\n",
    "        #update the \n",
    "        test_X = next_char_one_hot.reshape((1, char_size))\n",
    "\n",
    "    print(test_generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
