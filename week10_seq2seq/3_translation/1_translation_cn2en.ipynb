{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f) #,encoding='latin1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## read all data into variables \n",
    "X,Y,X_max_length,Y_max_length,en_int_to_vocab,en_vocab_to_int,cn_int_to_vocab,cn_vocab_to_int = read_dataset('cn2en.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence in English - encoded: [304, 16, 8, 7, 35, 14, 268, 1818, 15, 639, 6, 4, 32733, 5, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Sentence in Chinese - encoded: [2, 607, 9, 7, 91, 31, 655, 381, 6, 382, 87, 1287, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Decoded:\n",
      "------------------------\n",
      " Not that I ' m a big fan of yours ,  Blondie .  <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      " <go> 倒 不 是 因 为 支 持 你 金 发 妞 <eos> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "# inspect data\n",
    "print ('Sentence in English - encoded:', X[2])\n",
    "print ('Sentence in Chinese - encoded:', Y[2])\n",
    "print ('Decoded:\\n------------------------')\n",
    "\n",
    "s=\"\"\n",
    "for i in range(len(X_train[2])):\n",
    "    s = s + \" \" + en_int_to_vocab[X[2][i]]\n",
    "print(s)\n",
    "\n",
    "s=\"\"\n",
    "for i in range(len(Y_train[2])):\n",
    "    s = s + \" \" + cn_int_to_vocab[Y[2][i]]\n",
    "print (s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data padding\n",
    "\n",
    "def data_padding(x, y, length = 15):\n",
    "    for i in range(len(x)):\n",
    "        x[i] = x[i] + (length - len(x[i])) * [en_vocab_to_int['<pad>']]\n",
    "        y[i] = [cn_vocab_to_int['<go>']] + y[i] + [cn_vocab_to_int['<eos>']] + (length-len(y[i])) * [cn_vocab_to_int['<pad>']]\n",
    "\n",
    "input_length = min(max(X_max_length,Y_max_length),50)\n",
    "        \n",
    "data_padding(X, Y,input_length)\n",
    "\n",
    "# data splitting\n",
    "X_train,  X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1)\n",
    "\n",
    "del X\n",
    "del Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build a model\n",
    "\n",
    "input_seq_len = input_length\n",
    "output_seq_len = input_length+2\n",
    "en_vocab_size = len(en_vocab_to_int)# + 2 # + <pad>, <ukn>\n",
    "cn_vocab_size = len(cn_vocab_to_int)# + 4 # + <pad>, <ukn>, <eos>, <go>\n",
    "\n",
    "# placeholders\n",
    "encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "targets = [decoder_inputs[i+1] for i in range(output_seq_len-1)]\n",
    "targets.append(tf.placeholder(dtype = tf.int32, shape = [None], name = 'last_target')) ## add last, to make it the same as decode input\n",
    "target_weights = [tf.placeholder(dtype = tf.float32, shape = [None], name = 'target_w{}'.format(i)) for i in range(output_seq_len)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output projection\n",
    "size = 512\n",
    "w_t = tf.get_variable('proj_w', [cn_vocab_size, size], tf.float32)\n",
    "b = tf.get_variable('proj_b', [cn_vocab_size], tf.float32)\n",
    "w = tf.transpose(w_t)\n",
    "output_projection = (w, b)\n",
    "\n",
    "outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                            encoder_inputs,\n",
    "                                            decoder_inputs,\n",
    "                                            tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                            num_encoder_symbols = en_vocab_size,\n",
    "                                            num_decoder_symbols = cn_vocab_size,\n",
    "                                            embedding_size = 100,\n",
    "                                            feed_previous = False,\n",
    "                                            output_projection = output_projection,\n",
    "                                            dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our loss function\n",
    "\n",
    "# sampled softmax loss - returns: A batch_size 1-D tensor of per-example sampled softmax losses\n",
    "def sampled_loss(labels, logits):\n",
    "    return tf.nn.sampled_softmax_loss(\n",
    "                        weights = w_t,\n",
    "                        biases = b,\n",
    "                        labels = tf.reshape(labels, [-1, 1]),\n",
    "                        inputs = logits,\n",
    "                        num_sampled = 512,\n",
    "                        num_classes = cn_vocab_size)\n",
    "\n",
    "# Weighted cross-entropy loss for a sequence of logits\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss(outputs, targets, target_weights, softmax_loss_function = sampled_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's define some helper functions\n",
    "\n",
    "# simple softmax function\n",
    "def softmax(x):\n",
    "    n = np.max(x)\n",
    "    e_x = np.exp(x - n)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# feed data into placeholders\n",
    "def feed_dict(x, y, batch_size = 64):\n",
    "    feed = {}\n",
    "    idxes = np.random.choice(len(x), size = batch_size, replace = False)\n",
    "    for i in range(input_seq_len):\n",
    "        feed[encoder_inputs[i].name] = np.array([x[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    for i in range(output_seq_len):\n",
    "        feed[decoder_inputs[i].name] = np.array([y[j][i] for j in idxes], dtype = np.int32)\n",
    "        \n",
    "    feed[targets[len(targets)-1].name] = np.full(shape = [batch_size], fill_value = cn_vocab_to_int['<pad>'], dtype = np.int32)\n",
    "    \n",
    "    for i in range(output_seq_len-1):\n",
    "        batch_weights = np.ones(batch_size, dtype = np.float32)\n",
    "        target = feed[decoder_inputs[i+1].name]\n",
    "        for j in range(batch_size):\n",
    "            if target[j] == cn_vocab_to_int['<pad>']:\n",
    "                batch_weights[j] = 0.0\n",
    "        feed[target_weights[i].name] = batch_weights\n",
    "        \n",
    "    feed[target_weights[output_seq_len-1].name] = np.zeros(batch_size, dtype = np.float32)\n",
    "    \n",
    "    return feed\n",
    "\n",
    "# decode output sequence\n",
    "def decode_output(output_seq):\n",
    "    words = []\n",
    "    for i in range(output_seq_len):\n",
    "        smax = softmax(output_seq[i])\n",
    "        idx = np.argmax(smax)\n",
    "        words.append(cn_vocab_to_int[idx])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ops and hyperparameters\n",
    "learning_rate = 5e-3\n",
    "batch_size = 64\n",
    "steps = 1000\n",
    "\n",
    "# ops for projecting outputs\n",
    "outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "# training op\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# init op\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# forward step\n",
    "def forward_step(sess, feed):\n",
    "    output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "    return output_sequences\n",
    "\n",
    "# training step\n",
    "def backward_step(sess, feed):\n",
    "    sess.run(optimizer, feed_dict = feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------TRAINING------------------\n",
      "step: 0, loss: 7.844417095184326\n",
      "step: 4, loss: 7.757256984710693\n",
      "step: 9, loss: 7.768933296203613\n",
      "step: 14, loss: 7.752249717712402\n",
      "step: 19, loss: 7.780155658721924\n",
      "Checkpoint is saved\n",
      "step: 24, loss: 7.758981227874756\n",
      "step: 29, loss: 7.7191853523254395\n",
      "step: 34, loss: 7.602785110473633\n",
      "step: 39, loss: 7.5056915283203125\n",
      "step: 44, loss: 7.150213241577148\n",
      "step: 49, loss: 6.756635665893555\n",
      "step: 54, loss: 6.347491264343262\n",
      "step: 59, loss: 6.025028705596924\n",
      "step: 64, loss: 5.988553047180176\n",
      "step: 69, loss: 5.777231693267822\n",
      "step: 74, loss: 5.539206504821777\n",
      "step: 79, loss: 5.386470794677734\n",
      "step: 84, loss: 5.585608959197998\n",
      "step: 89, loss: 5.311697959899902\n",
      "step: 94, loss: 5.260750770568848\n",
      "step: 99, loss: 5.564685344696045\n",
      "step: 104, loss: 5.3327789306640625\n",
      "step: 109, loss: 5.518828868865967\n",
      "step: 114, loss: 5.326433181762695\n",
      "step: 119, loss: 5.004836559295654\n",
      "step: 124, loss: 6.112456321716309\n",
      "step: 129, loss: 5.058322429656982\n",
      "step: 134, loss: 5.896655559539795\n",
      "step: 139, loss: 5.289010524749756\n",
      "step: 144, loss: 4.8708038330078125\n",
      "step: 149, loss: 5.160789489746094\n",
      "step: 154, loss: 5.083125114440918\n",
      "step: 159, loss: 5.352367401123047\n",
      "step: 164, loss: 4.907405853271484\n",
      "step: 169, loss: 4.947977066040039\n",
      "step: 174, loss: 5.078241348266602\n",
      "step: 179, loss: 5.006430625915527\n",
      "step: 184, loss: 5.163846015930176\n",
      "step: 189, loss: 5.520264625549316\n",
      "step: 194, loss: 4.852474212646484\n",
      "step: 199, loss: 5.301591873168945\n",
      "step: 204, loss: 4.812193393707275\n",
      "step: 209, loss: 4.901180267333984\n",
      "step: 214, loss: 4.891709804534912\n",
      "step: 219, loss: 5.0409345626831055\n",
      "Checkpoint is saved\n",
      "step: 224, loss: 5.127974510192871\n",
      "step: 229, loss: 4.692145824432373\n",
      "step: 234, loss: 4.80091667175293\n",
      "step: 239, loss: 4.581451416015625\n",
      "step: 244, loss: 4.851916313171387\n",
      "step: 249, loss: 4.715611457824707\n",
      "step: 254, loss: 4.704102516174316\n",
      "step: 259, loss: 4.5672125816345215\n",
      "step: 264, loss: 4.516796112060547\n",
      "step: 269, loss: 4.6366376876831055\n",
      "step: 274, loss: 4.686756134033203\n",
      "step: 279, loss: 4.544816970825195\n",
      "step: 284, loss: 4.588339805603027\n",
      "step: 289, loss: 4.346391201019287\n",
      "step: 294, loss: 4.535494804382324\n",
      "step: 299, loss: 4.535957336425781\n",
      "step: 304, loss: 4.51165771484375\n",
      "step: 309, loss: 4.352943420410156\n",
      "step: 314, loss: 4.17202091217041\n",
      "step: 319, loss: 4.130075454711914\n",
      "step: 324, loss: 4.253333568572998\n",
      "step: 329, loss: 4.301671981811523\n",
      "step: 334, loss: 4.255361080169678\n",
      "step: 339, loss: 4.279085636138916\n",
      "step: 344, loss: 4.172809600830078\n",
      "step: 349, loss: 4.083468437194824\n",
      "step: 354, loss: 4.205392837524414\n",
      "step: 359, loss: 4.074196815490723\n",
      "step: 364, loss: 4.053455352783203\n",
      "step: 369, loss: 4.2571187019348145\n",
      "step: 374, loss: 4.225682258605957\n",
      "step: 379, loss: 4.12900972366333\n",
      "step: 384, loss: 3.917635917663574\n",
      "step: 389, loss: 3.9113056659698486\n",
      "step: 394, loss: 3.8830928802490234\n",
      "step: 399, loss: 3.921635150909424\n",
      "step: 404, loss: 3.729962110519409\n",
      "step: 409, loss: 3.9484176635742188\n",
      "step: 414, loss: 3.7309517860412598\n",
      "step: 419, loss: 3.7810544967651367\n",
      "Checkpoint is saved\n",
      "step: 424, loss: 3.9067487716674805\n",
      "step: 429, loss: 3.8562421798706055\n",
      "step: 434, loss: 3.8519651889801025\n",
      "step: 439, loss: 3.818350076675415\n",
      "step: 444, loss: 3.990799903869629\n",
      "step: 449, loss: 3.723740339279175\n",
      "step: 454, loss: 3.72261905670166\n",
      "step: 459, loss: 3.764455795288086\n",
      "step: 464, loss: 3.7703452110290527\n",
      "step: 469, loss: 3.6935057640075684\n",
      "step: 474, loss: 3.6025943756103516\n",
      "step: 479, loss: 3.809471845626831\n",
      "step: 484, loss: 3.678173542022705\n",
      "step: 489, loss: 3.570753574371338\n",
      "step: 494, loss: 3.5445055961608887\n",
      "step: 499, loss: 3.5370659828186035\n",
      "step: 504, loss: 3.556622266769409\n",
      "step: 509, loss: 3.627211570739746\n",
      "step: 514, loss: 3.374837875366211\n",
      "step: 519, loss: 3.598231315612793\n",
      "step: 524, loss: 3.7661149501800537\n",
      "step: 529, loss: 3.602720022201538\n",
      "step: 534, loss: 3.5261478424072266\n",
      "step: 539, loss: 3.617594003677368\n",
      "step: 544, loss: 3.6050920486450195\n",
      "step: 549, loss: 3.701080322265625\n",
      "step: 554, loss: 3.49287748336792\n",
      "step: 559, loss: 3.3718221187591553\n",
      "step: 564, loss: 3.6131701469421387\n",
      "step: 569, loss: 3.1961381435394287\n",
      "step: 574, loss: 3.6472110748291016\n",
      "step: 579, loss: 3.4449496269226074\n",
      "step: 584, loss: 3.6379172801971436\n",
      "step: 589, loss: 3.4750049114227295\n",
      "step: 594, loss: 3.417069673538208\n",
      "step: 599, loss: 3.280930757522583\n",
      "step: 604, loss: 3.4584226608276367\n",
      "step: 609, loss: 3.44360089302063\n",
      "step: 614, loss: 3.509492874145508\n",
      "step: 619, loss: 3.2075376510620117\n",
      "Checkpoint is saved\n",
      "step: 624, loss: 3.4213454723358154\n",
      "step: 629, loss: 3.44289493560791\n",
      "step: 634, loss: 3.290363073348999\n",
      "step: 639, loss: 3.3616979122161865\n",
      "step: 644, loss: 3.444967746734619\n",
      "step: 649, loss: 3.4111757278442383\n",
      "step: 654, loss: 3.413144111633301\n",
      "step: 659, loss: 3.558645725250244\n",
      "step: 664, loss: 3.2907185554504395\n",
      "step: 669, loss: 3.466853141784668\n",
      "step: 674, loss: 3.1555540561676025\n",
      "step: 679, loss: 3.117833137512207\n",
      "step: 684, loss: 3.185901165008545\n",
      "step: 689, loss: 3.269909620285034\n",
      "step: 694, loss: 3.2137527465820312\n",
      "step: 699, loss: 3.115119218826294\n",
      "step: 704, loss: 3.237616539001465\n",
      "step: 709, loss: 2.882035732269287\n",
      "step: 714, loss: 3.2646656036376953\n",
      "step: 719, loss: 3.136762857437134\n",
      "step: 724, loss: 3.0847344398498535\n",
      "step: 729, loss: 3.1461682319641113\n",
      "step: 734, loss: 3.2152152061462402\n",
      "step: 739, loss: 3.1753127574920654\n",
      "step: 744, loss: 3.1023993492126465\n",
      "step: 749, loss: 3.117915153503418\n",
      "step: 754, loss: 2.8683409690856934\n",
      "step: 759, loss: 3.1953091621398926\n",
      "step: 764, loss: 2.9791059494018555\n",
      "step: 769, loss: 3.0882372856140137\n",
      "step: 774, loss: 3.1436688899993896\n",
      "step: 779, loss: 3.2233502864837646\n",
      "step: 784, loss: 3.0682687759399414\n",
      "step: 789, loss: 3.2027463912963867\n",
      "step: 794, loss: 3.3978347778320312\n",
      "step: 799, loss: 3.0662448406219482\n",
      "step: 804, loss: 3.3004140853881836\n",
      "step: 809, loss: 2.8852806091308594\n",
      "step: 814, loss: 3.059906244277954\n",
      "step: 819, loss: 2.875154972076416\n",
      "Checkpoint is saved\n",
      "step: 824, loss: 3.0286474227905273\n",
      "step: 829, loss: 2.9661548137664795\n",
      "step: 834, loss: 2.813802480697632\n",
      "step: 839, loss: 2.9824724197387695\n",
      "step: 844, loss: 3.006687879562378\n",
      "step: 849, loss: 3.0854737758636475\n",
      "step: 854, loss: 3.0858547687530518\n",
      "step: 859, loss: 2.8712825775146484\n",
      "step: 864, loss: 3.135225772857666\n",
      "step: 869, loss: 2.838247776031494\n",
      "step: 874, loss: 2.952249050140381\n",
      "step: 879, loss: 3.0715184211730957\n",
      "step: 884, loss: 3.0124905109405518\n",
      "step: 889, loss: 2.900977373123169\n",
      "step: 894, loss: 2.959885597229004\n",
      "step: 899, loss: 2.907891035079956\n",
      "step: 904, loss: 2.94626522064209\n",
      "step: 909, loss: 3.085557460784912\n",
      "step: 914, loss: 3.2563223838806152\n",
      "step: 919, loss: 2.63202166557312\n",
      "step: 924, loss: 2.8194074630737305\n",
      "step: 929, loss: 2.7015457153320312\n",
      "step: 934, loss: 2.7550270557403564\n",
      "step: 939, loss: 2.7959187030792236\n",
      "step: 944, loss: 2.994058132171631\n",
      "step: 949, loss: 2.5703251361846924\n",
      "step: 954, loss: 2.7878241539001465\n",
      "step: 959, loss: 2.784039258956909\n",
      "step: 964, loss: 2.778109073638916\n",
      "step: 969, loss: 2.7697997093200684\n",
      "step: 974, loss: 2.856309413909912\n",
      "step: 979, loss: 3.0288705825805664\n",
      "step: 984, loss: 2.8057332038879395\n",
      "step: 989, loss: 2.8388993740081787\n",
      "step: 994, loss: 2.894803285598755\n",
      "step: 999, loss: 2.987731456756592\n",
      "Training time for 1000 steps: 5891.932959794998s\n"
     ]
    }
   ],
   "source": [
    "# let's train the model\n",
    "\n",
    "# we will use this list to plot losses through steps\n",
    "losses = []\n",
    "\n",
    "# save a checkpoint so we can restore the model later \n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print ('------------------TRAINING------------------')\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    t = time.time()\n",
    "    for step in range(steps):\n",
    "        feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "        backward_step(sess, feed)\n",
    "        \n",
    "        if step % 5 == 4 or step == 0:\n",
    "            loss_value = sess.run(loss, feed_dict = feed)\n",
    "            print('step: {}, loss: {}'.format(step, loss_value))\n",
    "            losses.append(loss_value)\n",
    "        \n",
    "        if step % 200 == 19:\n",
    "            saver.save(sess, 'checkpoints/', global_step=step)\n",
    "            print ('Checkpoint is saved')\n",
    "            \n",
    "    print ('Training time for {} steps: {}s'.format(steps, time.time() - t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbEAAAEkCAYAAAChew9BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x16139d4a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot losses\n",
    "\n",
    "with plt.style.context('fivethirtyeight'):\n",
    "    plt.plot(losses, linewidth = 1)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Losses')\n",
    "    plt.ylim((0, 12))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-517016096f5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mfeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mfeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcn_vocab_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<go>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-517016096f5a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mfeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mfeed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdecoder_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcn_vocab_to_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<go>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_sentences_encoded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# let's test the model\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    \n",
    "    # placeholders\n",
    "    encoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'encoder{}'.format(i)) for i in range(input_seq_len)]\n",
    "    decoder_inputs = [tf.placeholder(dtype = tf.int32, shape = [None], name = 'decoder{}'.format(i)) for i in range(output_seq_len)]\n",
    "\n",
    "    # output projection\n",
    "    size = 512\n",
    "    w_t = tf.get_variable('proj_w', [cn_vocab_size, size], tf.float32)\n",
    "    b = tf.get_variable('proj_b', [cn_vocab_size], tf.float32)\n",
    "    w = tf.transpose(w_t)\n",
    "    output_projection = (w, b)\n",
    "    \n",
    "    # change the model so that output at time t can be fed as input at time t+1\n",
    "    outputs, states = tf.contrib.legacy_seq2seq.embedding_attention_seq2seq(\n",
    "                                                encoder_inputs,\n",
    "                                                decoder_inputs,\n",
    "                                                tf.contrib.rnn.BasicLSTMCell(size),\n",
    "                                                num_encoder_symbols = en_vocab_size,\n",
    "                                                num_decoder_symbols = cn_vocab_size,\n",
    "                                                embedding_size = 100,\n",
    "                                                feed_previous = True, # <-----this is changed----->\n",
    "                                                output_projection = output_projection,\n",
    "                                                dtype = tf.float32)\n",
    "    \n",
    "    # ops for projecting outputs\n",
    "    outputs_proj = [tf.matmul(outputs[i], output_projection[0]) + output_projection[1] for i in range(output_seq_len)]\n",
    "\n",
    "    # let's translate these sentences     \n",
    "    en_sentences = [\"What' s your name\", 'My name is', 'What are you doing', 'I am reading a book',\\\n",
    "                    'How are you', 'I am good', 'Do you speak English', 'What time is it', 'Hi', 'Goodbye', 'Yes', 'No']\n",
    "    en_sentences_encoded = [[en_vocab_to_int.get(word, 0) for word in en_sentence.split()] for en_sentence in en_sentences]\n",
    "    en_vocab_to_int\n",
    "    # padding to fit encoder input\n",
    "    for i in range(len(en_sentences_encoded)):\n",
    "        en_sentences_encoded[i] += (15 - len(en_sentences_encoded[i])) * [en_vocab_to_int['<pad>']]\n",
    "    \n",
    "    # restore all variables - use the last checkpoint saved\n",
    "    saver = tf.train.Saver()\n",
    "    path = tf.train.latest_checkpoint('checkpoints')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        # restore\n",
    "        saver.restore(sess, path)\n",
    "        \n",
    "        # feed data into placeholders\n",
    "        feed = {}\n",
    "        for i in range(input_seq_len):\n",
    "            feed[encoder_inputs[i].name] = np.array([en_sentences_encoded[j][i] for j in range(len(en_sentences_encoded))], dtype = np.int32)\n",
    "            \n",
    "        feed[decoder_inputs[0].name] = np.array([cn_vocab_to_int['<go>']] * len(en_sentences_encoded), dtype = np.int32)\n",
    "        \n",
    "        # translate\n",
    "        output_sequences = sess.run(outputs_proj, feed_dict = feed)\n",
    "        \n",
    "        # decode seq.\n",
    "        for i in range(len(en_sentences_encoded)):\n",
    "            print ('{}.\\n--------------------------------'.format(i+1))\n",
    "            ouput_seq = [output_sequences[j][i] for j in range(output_seq_len)]\n",
    "            #decode output sequence\n",
    "            words = decode_output(ouput_seq)\n",
    "        \n",
    "            print (en_sentences[i])\n",
    "            for i in range(len(words)):\n",
    "                if words[i] not in ['<eos>', '<pad>', '<go>']:\n",
    "                    print (words[i],)\n",
    "            \n",
    "            print ('\\n--------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This model can be improved by using more training steps, better dataset or even with better selection of hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
