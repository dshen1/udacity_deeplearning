{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Load data into list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../../data/10_translation/srt'\n",
    "data_cn_path = os.path.join(data_path,'cn')\n",
    "data_en_path = os.path.join(data_path,'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en_files_names = os.listdir(data_en_path)\n",
    "cn_files_names = os.listdir(data_en_path)\n",
    "en_files_dict = {int(f.split('_')[0]):f for f in en_files_names}\n",
    "cn_files_dict = {int(f.split('_')[0]):f for f in cn_files_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"0_'71.2014.BluRay.720p.x264.AC3.╖▒╠σ&╙ó╬─.txt\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_files_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## define a function to read certain number of fines and make them into a list\n",
    "def read_files(file_path,file_dict,num_files):\n",
    "    sentence_list = []\n",
    "    for i in range(num_files):\n",
    "        path = os.path.join(file_path,file_dict[i])\n",
    "        with open(path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            #print(len(lines))\n",
    "            sentence_list.extend(lines)\n",
    "            \n",
    "    return sentence_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## number of files we want to read \n",
    "num_files = 1000\n",
    "train_X = read_files(data_en_path,en_files_dict,num_files)\n",
    "train_Y = read_files(data_cn_path,cn_files_dict,num_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417205, 417205)\n"
     ]
    }
   ],
   "source": [
    "## check if lines are alined \n",
    "print((len(train_X),len(train_Y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's it! Into him, Hook!\n",
      "\n",
      "就是这样  靠近点  胡克\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x = 1\n",
    "print(train_X[x])\n",
    "print(train_Y[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 Clean up data  - create vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_WORD_SPLIT = re.compile(r\"([.,!?\\\"':;])\")                  ## make all punctuation into its' own token\n",
    "_MIX_SPLIT = re.compile(r\"[\\w\\d]|-+|[\\u4e00-\\ufaff]|[^\\s]\") ## keep all chinese and english words \n",
    "_DIGIT_RE = re.compile(r\"\\d+\")\n",
    "\n",
    "def en_tokenize_setence(se):\n",
    "    words = []\n",
    "    for s in se.split():\n",
    "        words.extend(_WORD_SPLIT.split(s))\n",
    "    return words\n",
    "\n",
    "def cn_tokenize_setence(se):\n",
    "    return _MIX_SPLIT.findall(se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_sents = [en_tokenize_setence(x) for x in train_X]\n",
    "X_voc_counter = Counter([word for sentance in X_sents for word in sentance])\n",
    "X_voc = sorted(X_voc_counter, key=X_voc_counter.get, reverse=True)\n",
    "\n",
    "Y_sents = [cn_tokenize_setence(y) for y in train_Y]\n",
    "Y_voc_counter = Counter([word for sentance in Y_sents for word in sentance])\n",
    "Y_voc = sorted(Y_voc_counter, key=Y_voc_counter.get, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_words = ['<pad>', '<unk>', '<go>',  '<eos>']\n",
    "def get_voc_int_map(voc,special_words):\n",
    "    int_to_vocab = {word_i: word for word_i, word in enumerate(special_words + voc)}\n",
    "    vocab_to_int = {word: word_i for word_i, word in int_to_vocab.items()}\n",
    "    return int_to_vocab,vocab_to_int\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### get int voc map\n",
    "en_int_to_vocab,en_vocab_to_int = get_voc_int_map(X_voc,special_words)\n",
    "cn_int_to_vocab,cn_vocab_to_int = get_voc_int_map(Y_voc,special_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function to convert words to ids \n",
    "def sentence_to_token_ids(sentences, vocabulary):\n",
    "    letter_ids = [[vocabulary.get(letter,vocabulary['<unk>']) for letter in line] for line in sentences]\n",
    "    return letter_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = sentence_to_token_ids(X_sents, en_vocab_to_int)\n",
    "Y = sentence_to_token_ids(Y_sents, cn_vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_max_length = max([len(line) for line in X])\n",
    "Y_max_length = max([len(line) for line in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_dataset(file_path,obj):\n",
    "    with open(file_path,'wb') as f:\n",
    "        pickle.dump(obj,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj = (X,Y,X_max_length,Y_max_length,en_int_to_vocab,en_vocab_to_int,cn_int_to_vocab,cn_vocab_to_int)\n",
    "save_dataset('cn2en.p',obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "#X,Y,X_max_length,Y_max_length,en_int_to_vocab,en_vocab_to_int,cn_int_to_vocab,cn_vocab_to_int = read_dataset('cn2en.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
