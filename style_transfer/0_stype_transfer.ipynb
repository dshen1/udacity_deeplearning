{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer using VGG-16 Model\n",
    "\n",
    "<hr>\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We can maximize the feature activations inside a neural network so as to amplify patterns in the input image. This is called DeepDreaming.\n",
    "\n",
    "This notebook uses a similar idea but takes two images as input: A content-image and a style-image. We then wish to create a mixed-image which has the contours of the content-image and the colours and texture of the style-image.\n",
    "\n",
    "# Step 0 - what are we doing here?\n",
    "\n",
    "- the basic idea in 2015 by the german researchers was to repurpose a fully trained convolutional network to help trasnfer the style between two images.\n",
    "- we slice off the top layer since its used for classification and then \n",
    "- use the feature vectors at given layers to minimize the difference between two images\n",
    "- tensorflow 1.0, python 3, numpy and matplotlib\n",
    "\n",
    "# Step 1 - show style transfer examples for pics and videos\n",
    "\n",
    "- video style transfer paper http://genekogan.com/works/style-transfer/\n",
    "- introduces another loss function to minimize optical flow\n",
    "- Optical flow is the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene\n",
    "- show video style transfer and gradual transform vidoe at the bottom\n",
    "\n",
    "- Neural Doodle builds on this https://github.com/alexjc/neural-doodle creates semantic map using image segmentation\n",
    "\n",
    "# Step 2 - what is this process?\n",
    "\n",
    "- we want countour lines from content image\n",
    "- we want and textures from style image\n",
    "- whats in a filter? https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/\n",
    "- each conv layer has as many 3D filters as we set. could be 1. could be 20.\n",
    "- normally used to identify if features exist for classification\n",
    "- i.e minimize loss between labeled image and feature map output\n",
    "- each filter performs operation on input\n",
    "- outputs 3D feature map/activation map. \n",
    "- start with random noise for mixed image\n",
    "- calculate different loss functions at different layers\n",
    "- weigh these loss functions respectively\n",
    "- gradient of combined loss functions to update mixed image\n",
    "- we do this 100-1000 times until image is mixed\n",
    "\n",
    "\n",
    "content loss\n",
    "--------\n",
    "- calculate features/values at higher layer \n",
    "- minimize difference between activation features between content and  -- mixed image. calculate mean squared error. that is loss function. we want to minimize this.\n",
    "\n",
    "- we cache the feature from content image because we don't need to recalculate that. \n",
    "\n",
    "style loss\n",
    "--------\n",
    "- multiple layers \n",
    "- minimize difference between gram matrix for layer 1 and 2 for style and mixed image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Flowchart\n",
    "\n",
    "This flowchart shows roughly the idea of the Style Transfer algorithm, although we use the VGG-16 model which has many more layers than shown here.\n",
    "\n",
    "Two images are input to the neural network: A content-image and a style-image. We wish to generate the mixed-image which has the contours of the content-image and the colours and texture of the style-image.\n",
    "We do this by creating several loss-functions that can be optimized.\n",
    "\n",
    "The loss-function for the content-image tries to minimize the difference between the features that are activated for the content-image and for the mixed-image, at one or more layers in the network. This causes the contours of the mixed-image to resemble those of the content-image.\n",
    "\n",
    "The loss-function for the style-image is slightly more complicated, because it instead tries to minimize the difference between the so-called Gram-matrices for the style-image and the mixed-image. This is done at one or more layers in the network. The Gram-matrix measures which features are activated simultaneously in a given layer. Changing the mixed-image so that it mimics the activation patterns of the style-image causes the colour and texture to be transferred.\n",
    "\n",
    "We use TensorFlow to automatically derive the gradient for these loss-functions. The gradient is then used to update the mixed-image. This procedure is repeated a number of times until we are satisfied with the resulting image.\n",
    "\n",
    "There are some details of the Style Transfer algorithm not shown in this flowchart, e.g. regarding calculation of the Gram-matrices, calculation and storage of intermediate values for efficiency, a loss-function for denoising the mixed-image, and normalization of the loss-functions so they are easier to scale relative to each other.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/15_style_transfer_flowchart.png',height=70% width= 80%>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import PIL.Image\n",
    "from IPython.display import Image, display\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Model\n",
    "\n",
    "After having spent 2 days trying to get the style-transfer algorithm to work with the Inception 5h model that we used for DeepDreaming in Tutorial #14, I could not produce images that looked any good. This seems strange because the images that were produced in Tutorial #14 looked quite nice. But recall that we also used a few tricks to achieve that quality, such as smoothing the gradient and recursively downscaling and processing the image.\n",
    "\n",
    "The [original paper](https://arxiv.org/abs/1508.06576) on style transfer used the VGG-19 convolutional neural network. But the pre-trained VGG-19 models for TensorFlow did not seem suitable for this tutorial for different reasons. Instead we will use the VGG-16 model, which someone else has made available and which can easily be loaded in TensorFlow. We have wrapped it in a class for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading VGG16 Model ...\n",
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "import vgg16\n",
    "## download vgg model \n",
    "vgg16.maybe_download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper-functions for image manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads an image and returns it as a numpy array of floating-points. The image can be automatically resized so the largest of the height or width equals `max_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lod_image(filename,max_size=None):\n",
    "    image = PIL.Image.open(filename)\n",
    "    \n",
    "    if max_size is not None:\n",
    "        # Calculate the appropriate rescale-factor for ensuring a max height and width, while keeping\n",
    "        # the proportion between them.\n",
    "        factor = max_size / np.max(image.size)\n",
    "        # Scale the image's height and width.\n",
    "        size = np.array(image.size) * factor\n",
    "        size = size.astype(int)\n",
    "        image = image.resize(size,PIL.Image.LANCZOS)\n",
    "    ## convert to numpy floating-pint array \n",
    "    return np.float32(image)\n",
    "\n",
    "# Save an image as a jpeg-file. The image is given as a numpy array \n",
    "# with pixel-values between 0 and 255.\n",
    "def save_image(image, filename):\n",
    "    # Ensure the pixel-values are between 0 and 255.\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    # Convert to bytes.\n",
    "    image = image.astype(np.uint8)\n",
    "    # Write the image-file in jpeg-format.\n",
    "    with open(filename, 'wb') as file:\n",
    "        PIL.Image.fromarray(image).save(file, 'jpeg')\n",
    "\n",
    "# Plot a image \n",
    "def plot_image_big(image):\n",
    "    # Ensure the pixel-values are between 0 and 255.\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    # Convert pixels to bytes.\n",
    "    image = image.astype(np.uint8)\n",
    "    # Convert to a PIL-image and display it.\n",
    "    display(PIL.Image.fromarray(image))\n",
    "\n",
    "###############################################################\n",
    "### This function plots the content-, mixed- and style-images.\n",
    "###############################################################\n",
    "def plot_images(content_image, style_image, mixed_image):\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "    # Adjust vertical spacing.\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "    # Use interpolation to smooth pixels?\n",
    "    smooth = True\n",
    "    # Interpolation type.\n",
    "    if smooth:\n",
    "        interpolation = 'sinc'\n",
    "    else:\n",
    "        interpolation = 'nearest'\n",
    "\n",
    "    # Plot the content-image.\n",
    "    # Note that the pixel-values are normalized to\n",
    "    # the [0.0, 1.0] range by dividing with 255.\n",
    "    ax = axes.flat[0]\n",
    "    ax.imshow(content_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Content\")\n",
    "    # Plot the mixed-image.\n",
    "    ax = axes.flat[1]\n",
    "    ax.imshow(mixed_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Mixed\")\n",
    "    # Plot the style-image\n",
    "    ax = axes.flat[2]\n",
    "    ax.imshow(style_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Style\")\n",
    "    # Remove ticks from all the plots.\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Define lost functions\n",
    "This function creates a TensorFlow operation for calculating the Mean Squared Error between the two input tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## mean squared error\n",
    "def mean_squared_error(a, b):\n",
    "    return tf.reduce_mean(tf.square(a - b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This function creates the loss-function for the content-image. It is the Mean Squared Error of the feature activations in the given layers in the model, between the content-image and the mixed-image. When this content-loss is minimized, it therefore means that the mixed-image has feature activations in the given layers that are very similar to the activations of the content-image. Depending on which layers you select, this should transfer the contours from the content-image to the mixed-image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 3 - content image is 3d numpy array, indices for the layers\n",
    "#we want to use for content loss\n",
    "#you should expirment what looks good for different layers\n",
    "#there is not one best layer, we haven't found a way to minimize\n",
    "#loss for beauty. how to quantify?\n",
    "\n",
    "def create_content_loss(session,model,content_image,layer_ids):\n",
    "    '''\n",
    "    Create the loss-function for the content-image.\n",
    "    \n",
    "    Parameters:\n",
    "    session: An open TensorFlow session for running the model's graph.\n",
    "    model: The model, e.g. an instance of the VGG16-class.\n",
    "    content_image: Numpy float array with the content-image.\n",
    "    layer_ids: List of integer id's for the layers to use in the model.\n",
    "    '''\n",
    "    # A python dictionary object is generated with the \n",
    "    # placeholders as keys and the representative feed \n",
    "    # tensors as values.\n",
    "    # Create a feed-dict with the content-image.\n",
    "    feed_dict = model.create_feed_dict(image=content_image)\n",
    "    # Get references to the tensors for the given layers.\n",
    "    # collection of filters---------------------------- is this weights???\n",
    "    layers = model.get_layer_tensors(layer_ids)\n",
    "    # Calculate the output values of those layers when\n",
    "    # feeding the content-image to the model.\n",
    "    values = session.run(layers,feed_dict=feed_dict)\n",
    "    with model.graph.as_default():\n",
    "        # Initialize an empty list of loss-functions.\n",
    "        #because we are calculating losses per layer\n",
    "        layer_lossess=[]\n",
    "        # For each layer and its corresponding values\n",
    "        # for the content-image.\n",
    "        for value,layer in zip(values,layers):\n",
    "            # These are the values that are calculated\n",
    "            # for this layer in the model when inputting\n",
    "            # the content-image. Wrap it to ensure it\n",
    "            # is a const - although this may be done\n",
    "            # automatically by TensorFlow.\n",
    "            value_const = tf.constant(value)\n",
    "            ## take the mean square error of these two\n",
    "            loss = mean_squared_error(layer,value_const)\n",
    "            # list of loss-functions\n",
    "            layer_loss.append(loss)\n",
    "            \n",
    "        # The combined loss for all layers is just the average.\n",
    "        total_loss = tf.reduce_mean(layer_lossess)\n",
    "    \n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
