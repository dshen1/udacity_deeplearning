{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Wiki Text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Download data from https://metamind.io/research/the-wikitext-long-term-dependency-language-modeling-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#mini-demo\n",
    "from urllib.request import urlretrieve\n",
    "import os \n",
    "from os.path import isfile, isdir\n",
    "import zipfile \n",
    "from tqdm import tqdm\n",
    "import numpy as np #vectorization\n",
    "import random #generate probability distribution \n",
    "import tensorflow as tf #ml\n",
    "import datetime #clock training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First download data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exist\n"
     ]
    }
   ],
   "source": [
    "#### process bar\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "## download file \n",
    "data_path = './wikitext'\n",
    "if isdir(data_path):\n",
    "    print('Data already exist')\n",
    "else:\n",
    "    if not isdir(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    zip_file = os.path.join(data_path,'wikitext-103-v1.zip')\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='wikidata') as pbar:\n",
    "        urlretrieve('https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-103-v1.zip',\n",
    "                    zip_file,\n",
    "                    pbar.hook)\n",
    "    with zipfile.ZipFile(os.path.join(data_path,'wikitext-103-v1.zip')) as myzip:\n",
    "        myzip.extractall(data_path)\n",
    "    ## remove zip file \n",
    "    os.remove(data_path+'/wikitext-103-v1.zip')\n",
    "\n",
    "data_file_path = \"./wikitext/wikitext-103\"\n",
    "train_file = os.path.join(data_file_path,'wiki.train.tokens')\n",
    "validate_file = os.path.join(data_file_path,'wiki.valid.tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text length in number of characters: 538360726\n",
      "head of text:\n",
      " \n",
      " = Valkyria Chronicles III = \n",
      " \n",
      " Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . Employing the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      " The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomer\n"
     ]
    }
   ],
   "source": [
    "#lets open the text\n",
    "#native python file read function\n",
    "text = open('./wikitext/wikitext-103/wiki.train.tokens').read()\n",
    "print('text length in number of characters:', len(text))\n",
    "print('head of text:')\n",
    "print(text[:1000]) #all tokenized words, stored in a list called text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a id to character and character to id map dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of characters: 1250\n",
      "['\\n', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2']\n"
     ]
    }
   ],
   "source": [
    "## get the set of characters and sort them \n",
    "chars = sorted(list(set(text)))               ## all unique characters\n",
    "char_size = len(chars)\n",
    "print('number of characters:', char_size)\n",
    "print(chars[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## chrate char to id and id to char map \n",
    "char2id = {c:i for i,c in enumerate(chars)}\n",
    "id2char = {i:c for i,c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a probability of each character, return a likely character, one-hot encoded\n",
    "#our prediction will give us an array of probabilities of each character\n",
    "#we'll pick the most likely and one-hot encode it\n",
    "def sample(prediction):\n",
    "    '''\n",
    "    prediction: is a list of characters probilities\n",
    "    '''\n",
    "    r = random.uniform(0,1)  ## it is just a random number from 0-1\n",
    "    s = 0 \n",
    "    char_id = len(prediction)-1  ## this is because it starts with 0\n",
    "    #for each char prediction probability \n",
    "    for i in range(len(prediction)):\n",
    "        s+= prediction[i]\n",
    "        if s >= r:\n",
    "            char_id = i \n",
    "            break \n",
    "    \n",
    "    char_one_hot = np.zeros(shape[char_size])  ## one hot encode characters \n",
    "    char_one_hot[char_id] = 1.0\n",
    "    return char_one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X and y sets and one hot encode them  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## \n",
    "len_per_section = 50   ## \n",
    "skip = 2               ## shift over 2 characters for the result text\n",
    "sections = []\n",
    "next_chars = []\n",
    "\n",
    "for i in range(0,len(text)-len_per_section,skip):\n",
    "    sections.append(text[i:i+len_per_section]) ## grab a sequence of characters\n",
    "    next_chars.append(text[i+len_per_section]) ## this is the character acter previous sequence\n",
    "    \n",
    "## now we vectorize them \n",
    "#matrix of section length by num of characters\n",
    "X = np.zeros((len(sections), len_per_section, char_size))\n",
    "#label column for all the character id's, still zero\n",
    "y = np.zeros((len(sections), char_size))\n",
    "#for each char in each section, convert each char to an ID\n",
    "#for each section convert the labels to ids \n",
    "for i, section in enumerate(sections):\n",
    "    for j, char in enumerate(section):\n",
    "        X[i, j, char2id[char]] = 1          #this is where we do the one hot encoding\n",
    "    y[i, char2id[next_chars[i]]] = 1        \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this encoding method is not very good. takes a lot of memory. we may want to do it in tensorflow. \n",
    "That is how we did it in 0_basic lstm notebook "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
